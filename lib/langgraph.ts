import {
  AIMessage,
  BaseMessage,
  HumanMessage,
  SystemMessage,
  trimMessages,
} from '@langchain/core/messages'
import { ChatAnthropic } from '@langchain/anthropic'
import {
  END,
  MessagesAnnotation,
  START,
  StateGraph,
} from '@langchain/langgraph'
import { MemorySaver } from '@langchain/langgraph'
import { ToolNode } from '@langchain/langgraph/prebuilt'
import wxflows from '@wxflows/sdk/langchain'
import {
  ChatPromptTemplate,
  MessagesPlaceholder,
} from '@langchain/core/prompts'
import SYSTEM_MESSAGE from '@/constants/systemMessage'

// ëŒ€í™” ê¸°ë¡ì„ ê´€ë¦¬í•˜ê¸° ìœ„í•´ ë©”ì‹œì§€ë¥¼ ìë¦…ë‹ˆë‹¤.
const trimmer = trimMessages({
  maxTokens: 10,
  strategy: 'last',
  tokenCounter: (msgs) => msgs.length,

  includeSystem: true,
  allowPartial: false,
  startOn: 'human',
})

// wxflowsì— ì—°ê²°í•©ë‹ˆë‹¤.
const toolClient = new wxflows({
  endpoint: process.env.WXFLOWS_ENDPOINT || '',
  apikey: process.env.WXFLOWS_APIKEY,
})

// ë„êµ¬ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
const tools = await toolClient.lcTools
console.log('Available tools:', tools) // ë„êµ¬ë¥¼ í™•ì¸í•˜ê¸° ìœ„í•´ ì´ ì¤„ì„ ì¶”ê°€í•©ë‹ˆë‹¤.
const toolNode = new ToolNode(tools)

// ë” ë‚˜ì€ ë„êµ¬ ì§€ì¹¨ìœ¼ë¡œ LLM ì œê³µìì— ì—°ê²°í•©ë‹ˆë‹¤.
const initialiseModel = () => {
  const model = new ChatAnthropic({
    modelName: 'claude-3-5-sonnet-20241022',
    anthropicApiKey: process.env.ANTHROPIC_API_KEY,
    temperature: 0.7,
    maxTokens: 4096,
    streaming: true,
    clientOptions: {
      defaultHeaders: {
        'anthropic-beta': 'prompt-caching-2024-07-31',
      },
    },
    callbacks: [
      {
        handleLLMStart: async () => {
          // console.log("ğŸ¤– Starting LLM call");
        },
        handleLLMEnd: async (output) => {
          console.log('ğŸ¤– End LLM call', output)
          const usage = output.llmOutput?.usage
          if (usage) {
            console.log('ğŸ“Š Token Usage:', {
              input_tokens: usage.input_tokens,
              output_tokens: usage.output_tokens,
              total_tokens: usage.input_tokens + usage.output_tokens,
              cache_creation_input_tokens:
                usage.cache_creation_input_tokens || 0,
              cache_read_input_tokens: usage.cache_read_input_tokens || 0,
            })
          }
        },
        // handleLLMNewToken: async (token: string) => {
        //   // console.log("ğŸ”¤ New token:", token);
        // },
      },
    ],
  }).bindTools(tools)

  return model
}

// ê³„ì†í• ì§€ ì—¬ë¶€ë¥¼ ê²°ì •í•˜ëŠ” í•¨ìˆ˜ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.
function shouldContinue(state: typeof MessagesAnnotation.State) {
  const messages = state.messages
  const lastMessage = messages[messages.length - 1] as AIMessage

  // LLMì´ ë„êµ¬ í˜¸ì¶œì„ í•˜ë©´ "tools" ë…¸ë“œë¡œ ë¼ìš°íŒ…í•©ë‹ˆë‹¤.
  if (lastMessage.tool_calls?.length) {
    return 'tools'
  }

  // ë§ˆì§€ë§‰ ë©”ì‹œì§€ê°€ ë„êµ¬ ë©”ì‹œì§€ì¸ ê²½ìš° ì—ì´ì „íŠ¸ë¡œ ë‹¤ì‹œ ë¼ìš°íŒ…í•©ë‹ˆë‹¤.
  if (lastMessage.content && lastMessage._getType() === 'tool') {
    return 'agent'
  }

  // ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ ì¤‘ì§€í•©ë‹ˆë‹¤ (ì‚¬ìš©ìì—ê²Œ ì‘ë‹µ).
  return END
}

// ìƒˆë¡œìš´ ê·¸ë˜í”„ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.
const createWorkflow = () => {
  const model = initialiseModel()

  return new StateGraph(MessagesAnnotation)
    .addNode('agent', async (state) => {
      // ì‹œìŠ¤í…œ ë©”ì‹œì§€ ë‚´ìš©ì„ ìƒì„±í•©ë‹ˆë‹¤.
      const systemContent = SYSTEM_MESSAGE

      // ì‹œìŠ¤í…œ ë©”ì‹œì§€ì™€ ë©”ì‹œì§€ í”Œë ˆì´ìŠ¤í™€ë”ë¡œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì„ ìƒì„±í•©ë‹ˆë‹¤.
      const promptTemplate = ChatPromptTemplate.fromMessages([
        new SystemMessage(systemContent, {
          cache_control: { type: 'ephemeral' }, // ìºì‹œ ë¸Œë ˆì´í¬í¬ì¸íŠ¸ ì„¤ì •
        }),
        new MessagesPlaceholder('messages'), //
      ])

      // ëŒ€í™” ê¸°ë¡ì„ ê´€ë¦¬í•˜ê¸° ìœ„í•´ ë©”ì‹œì§€ë¥¼ ìë¦…ë‹ˆë‹¤.
      const trimmedMessages = await trimmer.invoke(state.messages)

      // í˜„ì¬ ë©”ì‹œì§€ë¡œ í”„ë¡¬í”„íŠ¸ë¥¼ í˜•ì‹í™”í•©ë‹ˆë‹¤.
      const prompt = await promptTemplate.invoke({ messages: trimmedMessages })

      // ëª¨ë¸ì—ì„œ ì‘ë‹µì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
      const response = await model.invoke(prompt)

      return { messages: [response] }
    })
    .addNode('tools', toolNode)
    .addEdge(START, 'agent')
    .addConditionalEdges('agent', shouldContinue)
    .addEdge('tools', 'agent')
}

// ìºì‹± í—¤ë”ë¥¼ ì¶”ê°€í•˜ëŠ” í•¨ìˆ˜ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.
function addCachingHeaders(messages: BaseMessage[]): BaseMessage[] {
  // í„´ë³„ ëŒ€í™”ì— ëŒ€í•œ ìºì‹± í—¤ë” ê·œì¹™
  // 1. ì²« ë²ˆì§¸ SYSTEM ë©”ì‹œì§€ë¥¼ ìºì‹œí•©ë‹ˆë‹¤.
  // 2. ë§ˆì§€ë§‰ ë©”ì‹œì§€ë¥¼ ìºì‹œí•©ë‹ˆë‹¤.
  // 3. ë‘ ë²ˆì§¸ë¡œ ë§ˆì§€ë§‰ HUMAN ë©”ì‹œì§€ë¥¼ ìºì‹œí•©ë‹ˆë‹¤.

  if (!messages.length) return messages

  // ì›ë³¸ì„ ë³€ê²½í•˜ì§€ ì•Šê¸° ìœ„í•´ ë©”ì‹œì§€ì˜ ë³µì‚¬ë³¸ì„ ë§Œë“­ë‹ˆë‹¤.
  const cachedMessages = [...messages]

  // ìºì‹œ ì œì–´ë¥¼ ì¶”ê°€í•˜ëŠ” ë„ìš°ë¯¸ í•¨ìˆ˜
  const addCache = (message: BaseMessage) => {
    message.content = [
      {
        type: 'text',
        text: message.content as string,
        cache_control: { type: 'ephemeral' },
      },
    ]
  }

  // ë§ˆì§€ë§‰ ë©”ì‹œì§€ë¥¼ ìºì‹œí•©ë‹ˆë‹¤.
  // console.log("ğŸ¤‘ğŸ¤‘ğŸ¤‘ Caching last message");
  addCache(cachedMessages.at(-1)!)

  // ë‘ ë²ˆì§¸ë¡œ ë§ˆì§€ë§‰ ì¸ê°„ ë©”ì‹œì§€ë¥¼ ì°¾ì•„ ìºì‹œí•©ë‹ˆë‹¤.
  let humanCount = 0
  for (let i = cachedMessages.length - 1; i >= 0; i--) {
    if (cachedMessages[i] instanceof HumanMessage) {
      humanCount++
      if (humanCount === 2) {
        // console.log("ğŸ¤‘ğŸ¤‘ğŸ¤‘ Caching second-to-last human message");
        addCache(cachedMessages[i])
        break
      }
    }
  }

  return cachedMessages
}

// ì§ˆë¬¸ì„ ì œì¶œí•˜ëŠ” í•¨ìˆ˜ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.
export async function submitQuestion(messages: BaseMessage[], chatId: string) {
  // ë©”ì‹œì§€ì— ìºì‹± í—¤ë”ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.
  const cachedMessages = addCachingHeaders(messages)
  // console.log("ğŸ”’ğŸ”’ğŸ”’ Messages:", cachedMessages);

  // chatIdì™€ onToken ì½œë°±ìœ¼ë¡œ ì›Œí¬í”Œë¡œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
  const workflow = createWorkflow()

  // ëŒ€í™” ìƒíƒœë¥¼ ì €ì¥í•˜ê¸° ìœ„í•´ ì²´í¬í¬ì¸íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
  const checkpointer = new MemorySaver()
  const app = workflow.compile({ checkpointer })

  const stream = await app.streamEvents(
    { messages: cachedMessages },
    {
      version: 'v2',
      configurable: { thread_id: chatId },
      streamMode: 'messages',
      runId: chatId,
    }
  )
  return stream
}
